name: 自动清洗节点 (Auto Clean)

on:
  workflow_dispatch:
  schedule:
    # UTC 22:00 = 澳门时间 06:00
    - cron: "0 22 * * *"

jobs:
  run_cleaner:
    runs-on: ubuntu-latest
    timeout-minutes: 40

    permissions:
      contents: read

    steps:
      - name: 1) Checkout docker branch
        uses: actions/checkout@v4
        with:
          ref: docker

      - name: 2) Start service (docker compose up)
        run: |
          set -e
          docker compose up -d --build
          docker compose ps
          docker ps

      - name: 3) Wait for API ready (/ipcheck)
        run: |
          echo "Waiting for service to be ready..."
          # 最多等待 180 秒（90 次 * 2 秒）
          for i in {1..90}; do
            if curl -fsS http://127.0.0.1:8000/ipcheck >/dev/null; then
              echo "Service ready."
              exit 0
            fi
            sleep 2
          done

          echo "Service did not become ready in time"
          echo "---- docker compose ps ----"
          docker compose ps || true
          echo "---- docker compose logs ----"
          docker compose logs --no-color || true
          exit 1

      - name: 4) Generate outputs for multiple subscriptions (tolerate 429)
        env:
          SUB_URLS: ${{ secrets.SUB_URLS }}
        run: |
          python - <<'PY'
          import os, re, time, sys, urllib.parse, subprocess, pathlib

          raw = os.environ.get("SUB_URLS", "").strip()
          if not raw:
              print("ERROR: secrets.SUB_URLS is empty. Put one subscription URL per line.", file=sys.stderr)
              sys.exit(1)

          urls = []
          for line in raw.splitlines():
              line = line.strip()
              if not line or line.startswith("#"):
                  continue
              urls.append(line)

          if not urls:
              print("ERROR: No valid URLs found in SUB_URLS.", file=sys.stderr)
              sys.exit(1)

          out_dir = pathlib.Path("outputs")
          out_dir.mkdir(exist_ok=True)

          # 用于判断“是否已注入标签”的关键词（出现任意一个就视为 tagged）
          keywords = [
              "机房", "原生", "广播",
              "风险", "极好", "高危", "极度风险",
              "ping0", "ippure",
              "Risk", "HIGH", "MED", "LOW"
          ]

          def is_tagged(text: str) -> bool:
              return any(k in text for k in keywords)

          def host_from_url(url: str) -> str:
              m = re.search(r"https?://([^/]+)", url)
              host = m.group(1) if m else "sub"
              return re.sub(r"[^a-zA-Z0-9._-]+", "_", host)

          # curl 拉取一次，并返回 http status code
          def curl_fetch(api: str, out_file: pathlib.Path) -> int:
              p = subprocess.run(
                  ["curl", "-sS", "-L", "-w", "%{http_code}", api, "-o", str(out_file)],
                  capture_output=True,
                  text=True
              )
              s = (p.stdout or "").strip()
              try:
                  return int(s[-3:])
              except:
                  return 0

          PER_SUB_TIMEOUT = 12 * 60   # 每个订阅最多等 12 分钟
          INTERVAL = 20               # 轮询间隔（秒）
          MAX_RETRY_429 = 6           # 429 最多重试次数

          failed = []

          for idx, sub in enumerate(urls, 1):
              host = host_from_url(sub)
              out_file = out_dir / f"cleaned_{idx:02d}_{host}.yaml"
              api = "http://127.0.0.1:8000/check?url=" + urllib.parse.quote(sub, safe="")

              print(f"\n=== [{idx}/{len(urls)}] Start: {host} ===")
              start = time.time()
              attempt = 0
              retry_429 = 0
              tagged = False

              while True:
                  attempt += 1
                  code = curl_fetch(api, out_file)

                  if code == 429:
                      retry_429 += 1
                      wait = min(120, 20 * retry_429)  # 温和退避
                      print(f"HTTP 429 (rate limited). retry {retry_429}/{MAX_RETRY_429}, wait {wait}s...")
                      if retry_429 >= MAX_RETRY_429:
                          print(f"SKIP: too many 429 for {host}")
                          break
                      time.sleep(wait)
                      continue

                  if code != 200:
                      print(f"HTTP {code} (not 200). sleep {INTERVAL}s and retry...")
                      if time.time() - start >= PER_SUB_TIMEOUT:
                          print("TIMEOUT while retrying non-200, keep last output.")
                          break
                      time.sleep(INTERVAL)
                      continue

                  # code == 200
                  content = out_file.read_text(encoding="utf-8", errors="ignore")
                  if is_tagged(content):
                      tagged = True
                      print(f"OK: tagged output detected (attempt {attempt}) -> {out_file.name}")
                      break

                  if time.time() - start >= PER_SUB_TIMEOUT:
                      print("TIMEOUT: not tagged in time, keep last output.")
                      break

                  print(f"Not tagged yet (attempt {attempt}), sleep {INTERVAL}s...")
                  time.sleep(INTERVAL)

              print(f"Saved: {out_file} | Tagged: {tagged}")

              # 如果文件不存在或为空，记录为失败（但不会让 job 失败）
              if (not out_file.exists()) or out_file.stat().st_size == 0:
                  failed.append(host)

          if failed:
              print("\nFAILED / SKIPPED subscriptions:", ", ".join(failed), file=sys.stderr)
          else:
              print("\nAll subscriptions processed.")

          PY

          echo "Generated files:"
          ls -lh outputs || true

      - name: 4.5) Collect debug info (always)
        if: always()
        run: |
          mkdir -p outputs
          echo "---- docker compose ps ----" > outputs/debug.txt
          docker compose ps >> outputs/debug.txt 2>&1 || true
          echo "" >> outputs/debug.txt
          echo "---- docker compose logs (tail 200) ----" >> outputs/debug.txt
          docker compose logs --no-color | tail -n 200 >> outputs/debug.txt 2>&1 || true

      - name: 5) Upload artifact (Artifacts)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cleaned-yaml
          path: |
            outputs/cleaned_*.yaml
            outputs/debug.txt
          retention-days: 14

      - name: 6) Shutdown
        if: always()
        run: |
          docker compose down -v
